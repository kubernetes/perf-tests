{{$PROMETHEUS_SCRAPE_KUBELETS := DefaultParam .PROMETHEUS_SCRAPE_KUBELETS false}}

apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    prometheus: k8s
  name: k8s
  namespace: monitoring
spec:
  logLevel: debug
  alerting:
    alertmanagers:
    - name: alertmanager-main
      namespace: monitoring
      port: web
  baseImage: gcr.io/k8s-testimages/quay.io/prometheus/prometheus
  nodeSelector:
    beta.kubernetes.io/os: linux
  replicas: 1
  resources:
    requests:
      cpu: 200m
      {{if $PROMETHEUS_SCRAPE_KUBELETS}}
      # TODO(oxddr): figure out memory limit
      memory: 10Gi # {{MultiplyInt 2 (AddInt 1 (DivideInt .Nodes 2000))}}Gi
      {{else}}
      # Start with 2Gi and add 2Gi for each 2K nodes.
      memory: {{MultiplyInt 2 (AddInt 1 (DivideInt .Nodes 2000))}}Gi
      {{end}}
    limits: 
      # Start with 500m and add 500m for each 1K nodes.
      cpu: {{MultiplyInt 500 (AddInt 1 (DivideInt .Nodes 1000))}}m
      {{if $PROMETHEUS_SCRAPE_KUBELETS}}
      # TODO(oxddr): figure out memory limit
      memory: 10Gi # {{MultiplyInt 2 (AddInt 1 (DivideInt .Nodes 2000))}}Gi
      {{else}}
      # Start with 2Gi and add 2Gi for each 2K nodes.
      memory: {{MultiplyInt 2 (AddInt 1 (DivideInt .Nodes 2000))}}Gi
      {{end}}
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  tolerations:
  - key: "monitoring"
    operator: "Exists"
    effect: "NoSchedule"
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  version: v2.9.2
  retention: 7d
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: ssd
        resources:
          requests:
            # Start with 10Gi, add 10Gi for each 1K nodes.
            storage: {{MultiplyInt 10 (AddInt 1 (DivideInt .Nodes 1000))}}Gi
  query:
    maxSamples: 100000000
