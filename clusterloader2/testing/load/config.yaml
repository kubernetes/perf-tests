# ASSUMPTIONS:
# - Underlying cluster should have 100+ nodes.
# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).
# - The number of created SVCs is half the number of created Deployments.
# - Only half of Deployments will be assigned 1-1 to existing SVCs.

#Constants
# Cater for the case where the number of nodes is less than nodes per namespace. See https://github.com/kubernetes/perf-tests/issues/887
{{$NODES_PER_NAMESPACE := MinInt .Nodes (DefaultParam .NODES_PER_NAMESPACE 100)}}
# See https://github.com/kubernetes/perf-tests/pull/1667#issuecomment-769642266
{{$IS_SMALL_CLUSTER := lt .Nodes 100}}
{{$PODS_PER_NODE := DefaultParam .PODS_PER_NODE 30}}
{{$LOAD_TEST_THROUGHPUT := DefaultParam .CL2_LOAD_TEST_THROUGHPUT 10}}
{{$DELETE_TEST_THROUGHPUT := DefaultParam .CL2_DELETE_TEST_THROUGHPUT $LOAD_TEST_THROUGHPUT}}
{{$BIG_GROUP_SIZE := DefaultParam .BIG_GROUP_SIZE 250}}
{{$MEDIUM_GROUP_SIZE := DefaultParam .MEDIUM_GROUP_SIZE 30}}
{{$SMALL_GROUP_SIZE := DefaultParam .SMALL_GROUP_SIZE 5}}
{{$SMALL_STATEFUL_SETS_PER_NAMESPACE := DefaultParam .SMALL_STATEFUL_SETS_PER_NAMESPACE 1}}
{{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE := DefaultParam .MEDIUM_STATEFUL_SETS_PER_NAMESPACE 1}}
{{$ENABLE_CHAOSMONKEY := DefaultParam .ENABLE_CHAOSMONKEY false}}
{{$ENABLE_API_AVAILABILITY_MEASUREMENT := DefaultParam .CL2_ENABLE_API_AVAILABILITY_MEASUREMENT false}}
#Variables
{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}
{{$totalPods := MultiplyInt $namespaces $NODES_PER_NAMESPACE $PODS_PER_NODE}}
{{$podsPerNamespace := DivideInt $totalPods $namespaces}}
{{$saturationTime := DivideInt $totalPods $LOAD_TEST_THROUGHPUT}}
{{$deletionTime := DivideInt $totalPods $DELETE_TEST_THROUGHPUT}}
# bigDeployments - 1/4 of namespace pods should be in big Deployments.
{{$bigDeploymentsPerNamespace := DivideInt $podsPerNamespace (MultiplyInt 4 $BIG_GROUP_SIZE)}}
# mediumDeployments - 1/4 of namespace pods should be in medium Deployments.
{{$mediumDeploymentsPerNamespace := DivideInt $podsPerNamespace (MultiplyInt 4 $MEDIUM_GROUP_SIZE)}}
# smallDeployments - 1/2 of namespace pods should be in small Deployments.
{{$smallDeploymentsPerNamespace := DivideInt $podsPerNamespace (MultiplyInt 2 $SMALL_GROUP_SIZE)}}

# Stateful sets are enabled. Reduce the number of small and medium deployments per namespace
# See https://github.com/kubernetes/perf-tests/issues/1036#issuecomment-607631768
# Ensure non zero or negative after subtraction.
{{$smallDeploymentsPerNamespace := MaxInt 0 (SubtractInt $smallDeploymentsPerNamespace $SMALL_STATEFUL_SETS_PER_NAMESPACE)}}
{{$mediumDeploymentsPerNamespace := MaxInt 0 (SubtractInt $mediumDeploymentsPerNamespace $MEDIUM_STATEFUL_SETS_PER_NAMESPACE)}}

# Jobs are enabled. Reduce the number of small, medium, big deployments per namespace.
# Ensure non zero or negative after subtraction.
{{$smallDeploymentsPerNamespace := MaxInt 0 (SubtractInt $smallDeploymentsPerNamespace 1)}}
{{$mediumDeploymentsPerNamespace := MaxInt 0 (SubtractInt $mediumDeploymentsPerNamespace 1)}}
{{$bigDeploymentsPerNamespace := MaxInt 0 (SubtractInt $bigDeploymentsPerNamespace 1)}}

# Disable big jobs on small clusters.
{{$bigJobsPerNamespace := IfThenElse $IS_SMALL_CLUSTER 0 1}}

# The minimal number of pods to be used to measure various things like
# pod-startup-latency or scheduler-throughput. The purpose of it is to avoid
# problems in small clusters where we wouldn't have enough samples (pods) to
# measure things accurately.
{{$MIN_PODS_IN_SMALL_CLUSTERS := 500}}

# BEGIN scheduler-throughput section
# TODO( https://github.com/kubernetes/perf-tests/issues/1027): Lower the number of "min-pods" once we fix the scheduler throughput measurement.
{{$totalSchedulerThroughputPods := MaxInt (MultiplyInt 2 $MIN_PODS_IN_SMALL_CLUSTERS) .Nodes}}
{{$schedulerThroughputPodsPerDeployment := .Nodes}}
{{$schedulerThroughputNamespaces := DivideInt $totalSchedulerThroughputPods $schedulerThroughputPodsPerDeployment}}
{{$schedulerThroughputThreshold := DefaultParam .CL2_SCHEDULER_THROUGHPUT_THRESHOLD 100}}
{{$ENABLE_HUGE_SERVICES := DefaultParam .CL2_ENABLE_HUGE_SERVICES false}}
# END scheduler-throughput section

# Set schedulerThroughputNamespaces to 1 on small clusters otherwise it will result
# in an unnecessary number of namespaces.
{{$schedulerThroughputNamespaces := IfThenElse $IS_SMALL_CLUSTER 1 $schedulerThroughputNamespaces}}

# Command to be executed
{{$EXEC_COMMAND := DefaultParam .CL2_EXEC_COMMAND nil}}
{{$EXIT_AFTER_EXEC := DefaultParam .CL2_EXIT_AFTER_EXEC false}}
{{$SLEEP_AFTER_EXEC_DURATION := DefaultParam .CL2_SLEEP_AFTER_EXEC_DURATION "0s"}}

name: load
namespace:
  number: {{AddInt $namespaces $schedulerThroughputNamespaces}}
tuningSets:
- name: Sequence
  parallelismLimitedLoad:
    parallelismLimit: 1
# Dedicated tuningSet for SchedulerThroughput phases that results in fully
# parallel creation of deployments.
- name: SchedulerThroughputParallel
  parallelismLimitedLoad:
    parallelismLimit: {{$schedulerThroughputNamespaces}}
# TODO(https://github.com/kubernetes/perf-tests/issues/1024): This TuningSet is used only for pod-startup-latency, get rid of it
- name: Uniform5qps
  qpsLoad:
    qps: 5
- name: RandomizedSaturationTimeLimited
  RandomizedTimeLimitedLoad:
    timeLimit: {{$saturationTime}}s
- name: RandomizedScalingTimeLimited
  RandomizedTimeLimitedLoad:
    # The expected number of created/deleted pods is totalPods/4 when scaling,
    # as each RS changes its size from X to a uniform random value in [X/2, 3X/2].
    # To match 10 [pods/s] requirement, we need to divide saturationTime by 4.
    timeLimit: {{DivideInt $saturationTime 4}}s
- name: RandomizedDeletionTimeLimited
  RandomizedTimeLimitedLoad:
    timeLimit: {{$deletionTime}}s
{{if $ENABLE_CHAOSMONKEY}}
chaosMonkey:
  nodeFailure:
    failureRate: 0.01
    interval: 5m
    jitterFactor: 2.0
    simulatedDowntime: 10m
{{end}}
steps:
- module:
    path: /modules/measurements.yaml
    params:
      action: start

- module:
    path: modules/services.yaml
    params:
      actionName: "Creating"
      namespaces: {{$namespaces}}
      smallServicesPerNamespace: {{DivideInt (AddInt $smallDeploymentsPerNamespace 1) 2}}
      mediumServicesPerNamespace: {{DivideInt (AddInt $mediumDeploymentsPerNamespace 1) 2}}
      bigServicesPerNamespace: {{DivideInt (AddInt $bigDeploymentsPerNamespace 1) 2}}

- name: Creating PriorityClass for DaemonSets
  phases:
  - replicasPerNamespace: 1
    tuningSet: Sequence
    objectBundle:
      - basename: daemonset-priorityclass
        objectTemplatePath: daemonset-priorityclass.yaml

- module:
    path: /modules/reconcile-objects.yaml
    params:
      actionName: "create"
      namespaces: {{$namespaces}}
      tuningSet: RandomizedSaturationTimeLimited
      daemonSetImage: k8s.gcr.io/pause:3.0
      daemonSetReplicas: 1
      bigDeploymentSize: {{$BIG_GROUP_SIZE}}
      bigDeploymentsPerNamespace: {{$bigDeploymentsPerNamespace}}
      mediumDeploymentSize: {{$MEDIUM_GROUP_SIZE}}
      mediumDeploymentsPerNamespace: {{$mediumDeploymentsPerNamespace}}
      smallDeploymentSize: {{$SMALL_GROUP_SIZE}}
      smallDeploymentsPerNamespace: {{$smallDeploymentsPerNamespace}}
      smallStatefulSetSize: {{$SMALL_GROUP_SIZE}}
      smallStatefulSetsPerNamespace: {{$SMALL_STATEFUL_SETS_PER_NAMESPACE}}
      mediumStatefulSetSize: {{$MEDIUM_GROUP_SIZE}}
      mediumStatefulSetsPerNamespace: {{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE}}
      bigJobSize: {{$BIG_GROUP_SIZE}}
      bigJobsPerNamespace: {{$bigJobsPerNamespace}}
      mediumJobSize: {{$MEDIUM_GROUP_SIZE}}
      mediumJobsPerNamespace: 1
      smallJobSize: {{$SMALL_GROUP_SIZE}}
      smallJobsPerNamespace: 1

{{if not $IS_SMALL_CLUSTER}}
# BEGIN scheduler throughput
- name: Creating scheduler throughput measurements
  measurements:
  - Identifier: HighThroughputPodStartupLatency
    Method: PodStartupLatency
    Params:
      action: start
      labelSelector: group = scheduler-throughput
      threshold: 1h # TODO(https://github.com/kubernetes/perf-tests/issues/1024): Ideally, this should be 5s
  - Identifier: WaitForSchedulerThroughputDeployments
    Method: WaitForControlledPodsRunning
    Params:
      action: start
      apiVersion: apps/v1
      kind: Deployment
      labelSelector: group = scheduler-throughput
      # The operation timeout shouldn't be less than 20m to make sure that ~10m node
      # failure won't fail the test. See https://github.com/kubernetes/kubernetes/issues/73461#issuecomment-467338711
      operationTimeout: 20m
  - Identifier: SchedulingThroughput
    Method: SchedulingThroughput
    Params:
      action: start
      labelSelector: group = scheduler-throughput
      measurmentInterval: 1s
{{if $ENABLE_HUGE_SERVICES}}
- name: Creating huge services
  phases:
  - namespaceRange:
      min: {{AddInt $namespaces 1}}
      max: {{AddInt $namespaces $schedulerThroughputNamespaces}}
    replicasPerNamespace: 1
    tuningSet: Sequence
    objectBundle:
    - basename: huge-service
      objectTemplatePath: service.yaml
{{end}}
- name: Creating scheduler throughput pods
  phases:
  - namespaceRange:
      min: {{AddInt $namespaces 1}}
      max: {{AddInt $namespaces $schedulerThroughputNamespaces}}
    replicasPerNamespace: 1
    tuningSet: SchedulerThroughputParallel
    objectBundle:
    - basename: scheduler-throughput-deployment
      objectTemplatePath: simple-deployment.yaml
      templateFillMap:
        Replicas: {{$schedulerThroughputPodsPerDeployment}}
        Group: scheduler-throughput
        CpuRequest: 1m
        MemoryRequest: 10M
{{if $ENABLE_HUGE_SERVICES}}
        SvcName: huge-service
{{end}}
- name: Waiting for scheduler throughput pods to be created
  measurements:
  - Identifier: WaitForSchedulerThroughputDeployments
    Method: WaitForControlledPodsRunning
    Params:
      action: gather
- name: Collecting scheduler throughput measurements
  measurements:
  - Identifier: HighThroughputPodStartupLatency
    Method: PodStartupLatency
    Params:
      action: gather
  - Identifier: SchedulingThroughput
    Method: SchedulingThroughput
    Params:
      action: gather
      enableViolations: true
      threshold: {{$schedulerThroughputThreshold}}
{{end}}

{{if $EXEC_COMMAND}}

{{if $ENABLE_API_AVAILABILITY_MEASUREMENT}}
- name: Pausing APIAvailability measurement
  measurements:
  - Identifier: APIAvailability
    Method: APIAvailability
    Params:
      action: pause
{{end}}

- name: Exec command
  measurements:
  - Identifier: ExecCommand
    Method: Exec
    Params:
      command:
      {{range $EXEC_COMMAND}}
      - {{.}}
      {{end}}

{{if $ENABLE_API_AVAILABILITY_MEASUREMENT}}
- name: Unpausing APIAvailability measurement
  measurements:
  - Identifier: APIAvailability
    Method: APIAvailability
    Params:
      action: unpause
{{end}}

- name: Sleep
  measurements:
  - Identifier: WaitAfterExec
    Method: Sleep
    Params:
      duration: {{$SLEEP_AFTER_EXEC_DURATION}}
{{end}}

{{if not $EXIT_AFTER_EXEC}}

{{if not $IS_SMALL_CLUSTER}}
- name: Deleting scheduler throughput pods
  phases:
  - namespaceRange:
      min: {{AddInt $namespaces 1}}
      max: {{AddInt $namespaces $schedulerThroughputNamespaces}}
    replicasPerNamespace: 0
    tuningSet: SchedulerThroughputParallel
    objectBundle:
    - basename: scheduler-throughput-deployment
      objectTemplatePath: simple-deployment.yaml
- name: Waiting for scheduler throughput pods to be deleted
  measurements:
  - Identifier: WaitForSchedulerThroughputDeployments
    Method: WaitForControlledPodsRunning
    Params:
      action: gather
{{if $ENABLE_HUGE_SERVICES}}
- name: Deleting huge services
  phases:
  - namespaceRange:
      min: {{AddInt $namespaces 1}}
      max: {{AddInt $namespaces $schedulerThroughputNamespaces}}
    replicasPerNamespace: 0
    tuningSet: Sequence
    objectBundle:
    - basename: huge-service
      objectTemplatePath: service.yaml
- name: Sleeping after deleting huge services
  measurements:
  - Identifier: WaitAfterHugeServicesDeletion
    Method: Sleep
    Params:
      duration: "3m"
{{end}}
# END scheduler throughput
{{end}}

{{if not $IS_SMALL_CLUSTER}}
# TODO(kubernetes/perf-tests/issues/1024): We shouldn't have a dedicated module for measuring pod-startup-latency.
- module:
    path: modules/pod-startup-latency.yaml
    params:
      namespaces: {{$namespaces}}
      minPodsInSmallCluster: {{$MIN_PODS_IN_SMALL_CLUSTERS}}
{{end}}

- module:
    path: /modules/reconcile-objects.yaml
    params:
      actionName: "scale and update"
      namespaces: {{$namespaces}}
      tuningSet: RandomizedScalingTimeLimited
      randomScaleFactor: 0.5
      daemonSetImage: k8s.gcr.io/pause:3.1
      daemonSetReplicas: 1
      bigDeploymentSize: {{$BIG_GROUP_SIZE}}
      bigDeploymentsPerNamespace: {{$bigDeploymentsPerNamespace}}
      mediumDeploymentSize: {{$MEDIUM_GROUP_SIZE}}
      mediumDeploymentsPerNamespace: {{$mediumDeploymentsPerNamespace}}
      smallDeploymentSize: {{$SMALL_GROUP_SIZE}}
      smallDeploymentsPerNamespace: {{$smallDeploymentsPerNamespace}}
      smallStatefulSetSize: {{$SMALL_GROUP_SIZE}}
      smallStatefulSetsPerNamespace: {{$SMALL_STATEFUL_SETS_PER_NAMESPACE}}
      mediumStatefulSetSize: {{$MEDIUM_GROUP_SIZE}}
      mediumStatefulSetsPerNamespace: {{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE}}
      bigJobSize: {{$BIG_GROUP_SIZE}}
      bigJobsPerNamespace: {{$bigJobsPerNamespace}}
      mediumJobSize: {{$MEDIUM_GROUP_SIZE}}
      mediumJobsPerNamespace: 1
      smallJobSize: {{$SMALL_GROUP_SIZE}}
      smallJobsPerNamespace: 1

- module:
    path: /modules/reconcile-objects.yaml
    params:
      actionName: "delete"
      namespaces: {{$namespaces}}
      tuningSet: RandomizedDeletionTimeLimited
      daemonSetImage: k8s.gcr.io/pause:3.1
      daemonSetReplicas: 0
      bigDeploymentSize: {{$BIG_GROUP_SIZE}}
      bigDeploymentsPerNamespace: 0
      mediumDeploymentSize: {{$MEDIUM_GROUP_SIZE}}
      mediumDeploymentsPerNamespace: 0
      smallDeploymentSize: {{$SMALL_GROUP_SIZE}}
      smallDeploymentsPerNamespace: 0
      smallStatefulSetSize: {{$SMALL_GROUP_SIZE}}
      smallStatefulSetsPerNamespace: 0
      mediumStatefulSetSize: {{$MEDIUM_GROUP_SIZE}}
      mediumStatefulSetsPerNamespace: 0
      bigJobSize: {{$BIG_GROUP_SIZE}}
      bigJobsPerNamespace: 0
      mediumJobSize: {{$MEDIUM_GROUP_SIZE}}
      mediumJobsPerNamespace: 0
      smallJobSize: {{$SMALL_GROUP_SIZE}}
      smallJobsPerNamespace: 0
      pvSmallStatefulSetSize: {{$SMALL_STATEFUL_SETS_PER_NAMESPACE}}
      pvMediumStatefulSetSize: {{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE}}

- name: Deleting PriorityClass for DaemonSets
  phases:
    - replicasPerNamespace: 0
      tuningSet: Sequence
      objectBundle:
        - basename: daemonset-priorityclass
          objectTemplatePath: daemonset-priorityclass.yaml

- module:
    path: modules/services.yaml
    params:
      actionName: "Deleting"
      namespaces: {{$namespaces}}
      smallServicesPerNamespace: 0
      mediumServicesPerNamespace: 0
      bigServicesPerNamespace: 0
{{end}} # not EXIT_AFTER_EXEC

- module:
    path: /modules/measurements.yaml
    params:
      action: gather
